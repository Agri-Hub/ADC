{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3f0820",
   "metadata": {},
   "source": [
    "<center><h2>Service 1: Feature Space Generation</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72689598",
   "metadata": {},
   "source": [
    "<img src='https://www.projectrhea.org/rhea/images/thumb/2/22/Hyperplane.png/700px-Hyperplane.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d43282",
   "metadata": {},
   "source": [
    "This notebook showcases how we can utilize the ADC in order to create a feature space based on parameters such as time range, bands and buffer zone for each parcel. In addition, a daily interpolation has been applied to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a138fa0",
   "metadata": {},
   "source": [
    "<b>Import Libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e038cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import datacube\n",
    "import math\n",
    "import calendar\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import Geometry,CRS\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import colors as mcolours\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import itertools\n",
    "from datetime import datetime,timedelta\n",
    "from pyproj import Proj, transform\n",
    "from osgeo import ogr\n",
    "from time import time\n",
    "import csv\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d2714",
   "metadata": {},
   "source": [
    "<b> Set the parameters </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5642e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2019'\n",
    "pilot = 'cyprus'\n",
    "region = 'new_area_1'\n",
    "buffer = 'buffer5'\n",
    "\n",
    "path_pilot = '/home/eouser/Desktop/jason_notebooks/shapefiles/{0}/{0}_declarations_{1}_3857.shp'.format(pilot,year)\n",
    "path_val = '/home/eouser/Desktop/jason_notebooks/shapefiles/{0}/crop_declarations_validations_{0}_{1}_3857.shp'.format(pilot,year)\n",
    "\n",
    "optical_bands = ['B02','B03','B04','B05','B06','B07','B08','B8A','B11','B12','ndvi','ndwi','psri']\n",
    "sar_bands = ['vv','vh']\n",
    "\n",
    "start_date,end_date = '2018-10-01','2019-07-01'\n",
    "d_break = 8 # number of breaks between the dates interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940eec7e",
   "metadata": {},
   "source": [
    "### Load both declarations and validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbf950b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327363, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>Decl_Code</th>\n",
       "      <th>Decl_Area</th>\n",
       "      <th>noa_id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000-21/31E2-C-156~1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((3716746.086 4190627.653, 3716705.329...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000-21/31E2-C-157~1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((3716746.086 4190627.653, 3716691.260...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000-21/31E2-C-158~1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((3716691.260 4190600.158, 3716653.480...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000-21/31E2-C-159~1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((3716554.931 4190768.499, 3716572.753...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000-21/31E2-C-160~3</td>\n",
       "      <td>3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>5</td>\n",
       "      <td>POLYGON ((3716405.532 4190777.355, 3716389.080...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              unique_id  Decl_Code  Decl_Area  noa_id  \\\n",
       "0  1000-21/31E2-C-156~1          1        8.0       1   \n",
       "1  1000-21/31E2-C-157~1          1        7.7       2   \n",
       "2  1000-21/31E2-C-158~1          1        9.6       3   \n",
       "3  1000-21/31E2-C-159~1          1       14.9       4   \n",
       "4  1000-21/31E2-C-160~3          3        6.9       5   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((3716746.086 4190627.653, 3716705.329...  \n",
       "1  POLYGON ((3716746.086 4190627.653, 3716691.260...  \n",
       "2  POLYGON ((3716691.260 4190600.158, 3716653.480...  \n",
       "3  POLYGON ((3716554.931 4190768.499, 3716572.753...  \n",
       "4  POLYGON ((3716405.532 4190777.355, 3716389.080...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbf = gpd.GeoDataFrame.from_file(path_pilot)\n",
    "print(dbf.shape)\n",
    "dbf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e502ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49095, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>Decl_Code</th>\n",
       "      <th>Decl_Name</th>\n",
       "      <th>Val_Code</th>\n",
       "      <th>Val_Name</th>\n",
       "      <th>NMA_C</th>\n",
       "      <th>Val_Mthd</th>\n",
       "      <th>SCAT_CROP_</th>\n",
       "      <th>SCAT_TREES</th>\n",
       "      <th>TREES_FOUN</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1012-21/61E1-E-87#1~42</td>\n",
       "      <td>42</td>\n",
       "      <td>ÎÎÎÎÎ£</td>\n",
       "      <td>42</td>\n",
       "      <td>ÎÎÎÎÎ£</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>POLYGON ((3711133.127 4184532.064, 3711133.451...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1012-21/61E1-E-87#2~42</td>\n",
       "      <td>42</td>\n",
       "      <td>ÎÎÎÎÎ£</td>\n",
       "      <td>42</td>\n",
       "      <td>ÎÎÎÎÎ£</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>POLYGON ((3711186.855 4184558.391, 3711188.728...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021-30/12W2-D-286~42</td>\n",
       "      <td>42</td>\n",
       "      <td>ÎÎÎÎÎ£</td>\n",
       "      <td>42</td>\n",
       "      <td>ÎÎÎÎÎ£</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>POLYGON ((3706845.565 4179624.019, 3706853.835...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1021-30/3W1-C-251~3</td>\n",
       "      <td>3</td>\n",
       "      <td>ÎÎ¡ÎÎÎÎ¡Î</td>\n",
       "      <td>3</td>\n",
       "      <td>ÎÎ¡ÎÎÎÎ¡Î</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MULTIPOLYGON (((3703935.067 4182076.450, 37039...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1023-30/31E1-F-281~3</td>\n",
       "      <td>3</td>\n",
       "      <td>ÎÎ¡ÎÎÎÎ¡Î</td>\n",
       "      <td>3</td>\n",
       "      <td>ÎÎ¡ÎÎÎÎ¡Î</td>\n",
       "      <td>1</td>\n",
       "      <td>OTS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((3716441.253 4176322.531, 3716457.369...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                unique_id  Decl_Code       Decl_Name  Val_Code  \\\n",
       "0  1012-21/61E1-E-87#1~42         42      ÎÎÎÎÎ£        42   \n",
       "1  1012-21/61E1-E-87#2~42         42      ÎÎÎÎÎ£        42   \n",
       "2   1021-30/12W2-D-286~42         42      ÎÎÎÎÎ£        42   \n",
       "3     1021-30/3W1-C-251~3          3  ÎÎ¡ÎÎÎÎ¡Î         3   \n",
       "4    1023-30/31E1-F-281~3          3  ÎÎ¡ÎÎÎÎ¡Î         3   \n",
       "\n",
       "         Val_Name  NMA_C Val_Mthd  SCAT_CROP_  SCAT_TREES  TREES_FOUN  \\\n",
       "0      ÎÎÎÎÎ£      1     None         NaN           0           8   \n",
       "1      ÎÎÎÎÎ£      1     None         NaN           0           8   \n",
       "2      ÎÎÎÎÎ£      1     None         NaN           0           8   \n",
       "3  ÎÎ¡ÎÎÎÎ¡Î      1     None         NaN           0           0   \n",
       "4  ÎÎ¡ÎÎÎÎ¡Î      1      OTS         0.0           0           0   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((3711133.127 4184532.064, 3711133.451...  \n",
       "1  POLYGON ((3711186.855 4184558.391, 3711188.728...  \n",
       "2  POLYGON ((3706845.565 4179624.019, 3706853.835...  \n",
       "3  MULTIPOLYGON (((3703935.067 4182076.450, 37039...  \n",
       "4  POLYGON ((3716441.253 4176322.531, 3716457.369...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbf_val = gpd.GeoDataFrame.from_file(path_val)\n",
    "print(dbf_val.shape)\n",
    "dbf_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea88069",
   "metadata": {},
   "outputs": [],
   "source": [
    "noa_to_id = {k:v for k,v in dbf[['noa_id','unique_id']].values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b9a54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49095\n"
     ]
    }
   ],
   "source": [
    "noa_ids_to_keep = sorted(dbf[dbf.unique_id.isin(dbf_val.unique_id)]['noa_id'].values)\n",
    "print(len(noa_ids_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b10079",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=\"/home/eouser/datacube.conf\"\n",
    "dc = datacube.Datacube(app=\"test\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b87db88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3760383.853569663, 3773903.405815693, 4157215.9410092006, 4169765.3027004637]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapefile_bbox = '/home/eouser/Desktop/jason_notebooks/shapefiles/{0}/{0}_bbox_{1}_3857.shp'.format(pilot,region)\n",
    "driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "ds = driver.Open(shapefile_bbox,0)\n",
    "layer = ds.GetLayer()\n",
    "xmin,xmax,ymin,ymax = layer.GetExtent()\n",
    "bbox = [xmin,xmax,ymin,ymax]\n",
    "bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d03aaf",
   "metadata": {},
   "source": [
    "### Load the ids for each parcel based on a layer that has been indexed on the cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1391275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_ids = 'ids_{}_{}'.format(pilot,year)\n",
    "\n",
    "def getIDs():\n",
    "    query = {\n",
    "        'product':product_ids,\n",
    "        'x':(xmin,xmax),\n",
    "        'y':(ymin,ymax),\n",
    "        'crs':'EPSG:3857'\n",
    "    }\n",
    "    dc = datacube.Datacube(app=\"test\", config=config)\n",
    "    data = dc.load(**query)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2011f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fields to check: 10236\n",
      "CPU times: user 408 ms, sys: 432 ms, total: 840 ms\n",
      "Wall time: 659 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ids = getIDs()\n",
    "print('number of fields to check: {}'.format(len(np.unique(ids[buffer][0].values))-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674da1c7",
   "metadata": {},
   "source": [
    "### Get the dates related to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd2f679c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2018-11-01', '2018-11-30'),\n",
       " ('2018-12-01', '2018-12-30'),\n",
       " ('2018-12-31', '2019-01-29'),\n",
       " ('2019-01-30', '2019-03-01'),\n",
       " ('2019-03-02', '2019-03-31'),\n",
       " ('2019-04-01', '2019-04-30'),\n",
       " ('2019-05-01', '2019-05-30'),\n",
       " ('2019-05-31', '2019-06-30')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def date_range(start, end, intv):\n",
    "\n",
    "    start = datetime.strptime(start,\"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end,\"%Y-%m-%d\")\n",
    "    diff = (end  - start ) / intv\n",
    "    for i in range(intv):\n",
    "        yield (start + diff * i).strftime(\"%Y-%m-%d\")\n",
    "    yield end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "dates_l = list(date_range(start_date,end_date,d_break))\n",
    "\n",
    "\n",
    "dates_list = []\n",
    "for i in range(len(dates_l)-1):\n",
    "    start,end = dates_l[i],dates_l[i+1]\n",
    "    start = datetime.strptime(start,\"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end,\"%Y-%m-%d\")\n",
    "    end = end - timedelta(1)\n",
    "    dates_list.append((start.strftime(\"%Y-%m-%d\"),end.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61beec5",
   "metadata": {},
   "source": [
    "### Creation of a hdf5 file to write in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b2a365b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"sar_bands\": shape (2,), type \"|S10\">"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf = h5py.File('/home/eouser/Desktop/jason_notebooks/fs/{0}/fs_{0}_{1}_{2}_{3}_raw.h5'.format(pilot,region,year,buffer), 'a')\n",
    "data = hf.create_group('data')\n",
    "sar_data = hf.create_group('sar_data')\n",
    "coords = hf.create_group('coords')\n",
    "meta = hf.create_group('metadata')\n",
    "meta.create_dataset('bands',(len(optical_bands)),'S10',[b.encode('ascii','ignore') for b in optical_bands])\n",
    "meta.create_dataset('sar_bands',(len(sar_bands)),'S10',[b.encode('ascii','ignore') for b in sar_bands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34474590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_index(data, index):\n",
    "    B02 = data.B02.astype('float16')\n",
    "    B03 = data.B03.astype('float16')\n",
    "    B04 = data.B04.astype('float16')\n",
    "    B05 = data.B05.astype('float16')\n",
    "    B06 = data.B06.astype('float16')\n",
    "    B07 = data.B07.astype('float16')\n",
    "    B08 = data.B08.astype('float16')\n",
    "    B8A = data.B8A.astype('float16')\n",
    "    B11 = data.B11.astype('float16')\n",
    "    B12 = data.B12.astype('float16')\n",
    "    if index.lower() == 'ndvi':\n",
    "        return (B08 - B04) / (B08 + B04)\n",
    "    if index.lower() == 'ndwi':\n",
    "        return (B03 - B08) / (B08 + B03)\n",
    "    if index.lower() == 'ndmi':\n",
    "        return (B08 - B11) / (B08 + B11)\n",
    "    if index.lower() == 'psri':\n",
    "        return (B04 - B02) / B06\n",
    "    if index.lower() == 'savi':\n",
    "        L = 0.428;\n",
    "        return ((B08 - B04) / (B08 + B04 + L)) * (1.0 + L)\n",
    "    if index.lower() == 'evi':\n",
    "        return 2.5 * (B08 - B04) / ((B08 + 6 * B04 - 7.5 * B02) + 1.0)\n",
    "    if index.lower() == 'dvi':\n",
    "        return (B08 - B04)\n",
    "    if index.lower() == 'rdvi':\n",
    "        return (B08 - B04) / (B08 + B04) ** 0.5\n",
    "    if index.lower() == 'rvi':\n",
    "        return (B08 / B04)\n",
    "    if index.lower() == 'tvi':\n",
    "        return (120 * (B08 - B03) - 200 * (B04 - B03)) / 2\n",
    "    if index.lower() == 'tcari':\n",
    "        return ((B08 - B04) - 0.2 * (B08 - B03) * (B08 / B04)) * 3\n",
    "    if index.lower() == 'gi':\n",
    "        return (B03 / B04)\n",
    "    if index.lower() == 'vigreen':\n",
    "        return (B03 - B04) / (B03 + B04)\n",
    "    if index.lower() == 'varigreen':\n",
    "        return (B03 - B04) / (B03 + B04 - B02)\n",
    "    if index.lower() == 'gari':\n",
    "        return (B08 - (B03 - (B02 - B04))) / (B08 - (B03 + (B02 - B04)))\n",
    "    if index.lower() == 'gdvi':\n",
    "        return (B08 - B03)\n",
    "    if index.lower() == 'sipi':\n",
    "        return (B08 - B02) / (B08 - B04)\n",
    "    if index.lower() == 'wdrvi':\n",
    "        alpha = 0.2\n",
    "        return (alpha * B08 - B04) / (alpha * B08 + B04)\n",
    "    if index.lower() == 'gvmi':\n",
    "        return ((B08 + 0.1) - (B12 + 0.02)) / ((B08 + 0.1) + (B12 + 0.02))\n",
    "    if index.lower() == 'gcvi':\n",
    "        return (B08 / B03) - 1\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def cloud_data(data, index):\n",
    "    return xr.where((data.SCL>=4) & (data.SCL<=6), data[index], np.nan)\n",
    "\n",
    "\n",
    "\n",
    "def getData_optical(bbox,timeStart,timeEnd,optical_bands,resolution = 10):\n",
    "    \n",
    "    product_optical= 's2_preprocessed_{}'.format(pilot)\n",
    "    \n",
    "    all_bands = ['B02','B03','B04','B05','B06','B07','B08','B8A','B11','B12','SCL']\n",
    "    all_indices = ['ndvi','ndwi','ndmi','psri','savi','evi','dvi','rdvi','rvi','tvi','tcari','gi','vigreen',\n",
    "                   'varigreen','gari','gdvi','sipi','wdrvi','gvmi','gcvi']\n",
    "    \n",
    "    bands = [b for b in optical_bands if b in all_bands]\n",
    "    indices = [b for b in optical_bands if b in all_indices]\n",
    "    \n",
    "    if bbox is not None:\n",
    "        xmin,xmax,ymin,ymax = bbox[0],bbox[1],bbox[2],bbox[3]\n",
    "    query = {\n",
    "        'time': (timeStart,timeEnd),\n",
    "        'product': product_optical,\n",
    "        'x':(xmin,xmax),\n",
    "        'y':(ymin,ymax),\n",
    "        'crs':'EPSG:3857'\n",
    "    }\n",
    "    dc = datacube.Datacube(app=\"test\", config=config)\n",
    "    data = dc.load(**query,measurements=all_bands,dask_chunks={})\n",
    "    for i,index in enumerate(optical_bands):\n",
    "        if index in indices:\n",
    "            data[index] = calculate_index(data,index)\n",
    "        data[index] = cloud_data(data,index)\n",
    "        if i == 0:\n",
    "#             to_keep = data[index].dropna(dim='time',how='all').time\n",
    "            to_keep = data[index].dropna(dim='time',thresh=0.25).time\n",
    "            data = data.sel(time=to_keep)\n",
    "\n",
    "    for b in all_bands:\n",
    "        if b not in bands:\n",
    "            data = data.drop(b)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e22fff",
   "metadata": {},
   "source": [
    "### Iterate over time and get pixel data for each parcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba997091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 8/8 [44:11<00:00, 331.42s/it]\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "\n",
    "for d_n in tqdm(range(len(dates_list))):\n",
    "     \n",
    "        \n",
    "    try:\n",
    "        d_start,d_end = dates_list[d_n][0],dates_list[d_n][1]\n",
    "        data_cube = getData_optical(bbox,d_start,d_end,optical_bands)\n",
    "        data_cube = data_cube.load()\n",
    "\n",
    "        ### important step: grouping of pixels per parcel based on id raster\n",
    "        grouped_data = data_cube.groupby(ids[buffer][0])\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    dates = data_cube.time.values\n",
    "    dates = np.array([str(t).split('T')[0] for t in dates])\n",
    "    dates_unique = sorted(set(dates))  \n",
    "\n",
    "    \n",
    "    all_keys = []\n",
    "    for f in grouped_data:\n",
    "        key, parcel_data = f[0],f[1]\n",
    "        if key!=-1:\n",
    "            coords_all = list(zip(parcel_data.x.values,parcel_data.y.values))\n",
    "            parcel_data = np.array([parcel_data[b].values for b in optical_bands])\n",
    "\n",
    "            if len(dates)!=dates_unique:\n",
    "                vals = []\n",
    "                for b in range(parcel_data.shape[0]):\n",
    "                    df = pd.DataFrame(data=parcel_data[b],index=dates)\n",
    "                    vals.append(df.groupby(df.index).mean().values)\n",
    "                vals = np.array(vals)\n",
    "            else:\n",
    "                vals = parcel_data.copy()\n",
    "            if flag:\n",
    "                coords.create_dataset(str(key),data=np.array(coords_all), compression='gzip',compression_opts=9)\n",
    "                data.create_dataset(str(key),data=vals.astype('float64'),compression='gzip',\n",
    "                                    compression_opts=9,maxshape=(vals.shape[0],None,vals.shape[2]))\n",
    "            else:\n",
    "                data[str(key)].resize((data[str(key)].shape[1] + vals.shape[1]), axis=1)\n",
    "                data[str(key)][:,-vals.shape[1]:,:] = vals\n",
    "            all_keys.append(key)\n",
    "\n",
    "    if flag:\n",
    "        meta.create_dataset('unique_ids',(len(all_keys)),'S40',\n",
    "                            [noa_to_id[i].encode('ascii','ignore') for i in np.array(sorted(all_keys))])\n",
    "        meta.create_dataset('dates',(len(dates_unique)),'S10',\n",
    "                            [d.encode('ascii','ignore') for d in dates_unique],maxshape=(None,))\n",
    "        flag = False\n",
    "    else:\n",
    "        meta['dates'].resize((meta['dates'].shape[0] + len(dates_unique)), axis=0)\n",
    "        meta['dates'][-len(dates_unique):] = dates_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5c7b7",
   "metadata": {},
   "source": [
    "### The same for SAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd41df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData_sar(bbox,timeStart,timeEnd,sar_bands,resolution = 10):\n",
    "    \n",
    "    product_sar= 'sentinel1_sar'\n",
    "    all_sar_bands = ['vv','vh']\n",
    "    \n",
    "    bands = [b for b in sar_bands if b in all_sar_bands]\n",
    "    if bbox is not None:\n",
    "        xmin,xmax,ymin,ymax = bbox[0],bbox[1],bbox[2],bbox[3]\n",
    "    query = {\n",
    "        'time': (timeStart,timeEnd),\n",
    "        'product': product_sar,\n",
    "        'x':(xmin,xmax),\n",
    "        'y':(ymin,ymax),\n",
    "        'crs':'EPSG:3857'\n",
    "    }\n",
    "    dc = datacube.Datacube(app=\"test\", config=config)\n",
    "    data = dc.load(**query,measurements=all_sar_bands,dask_chunks={})\n",
    "    for b in all_sar_bands:\n",
    "        if b not in bands:\n",
    "            data = data.drop(b)\n",
    "    return data\n",
    "\n",
    "flag = True\n",
    "\n",
    "if sar_bands:\n",
    "    \n",
    "    for d_n in tqdm(range(len(dates_list))):\n",
    "        \n",
    "        try:        \n",
    "            d_start,d_end = dates_list[d_n][0],dates_list[d_n][1]\n",
    "            data_cube = getData_sar(bbox,d_start,d_end,sar_bands)\n",
    "            data_cube = data_cube.load()\n",
    "            grouped_data = data_cube.groupby(ids[buffer][0])\n",
    "        except:\n",
    "            continue\n",
    "        sar_dates = data_cube.time.values\n",
    "        sar_dates = np.array([str(t).split('T')[0] for t in sar_dates])\n",
    "        dates_unique = sorted(set(sar_dates))  \n",
    "\n",
    "\n",
    "        for f in grouped_data:\n",
    "            key, parcel_data = f[0],f[1]\n",
    "#             if key in noa_ids_to_keep:\n",
    "            if key!=-1:\n",
    "                parcel_data = np.array([parcel_data[b].values for b in sar_bands])\n",
    "\n",
    "                if len(sar_dates)!=dates_unique:\n",
    "                    vals = []\n",
    "                    for b in range(parcel_data.shape[0]):\n",
    "                        df = pd.DataFrame(data=parcel_data[b],index=sar_dates)\n",
    "                        vals.append(df.groupby(df.index).mean().values)\n",
    "                    vals = np.array(vals)\n",
    "                else:\n",
    "                    vals = parcel_data.copy()\n",
    "                    \n",
    "                if flag:\n",
    "                    sar_data.create_dataset(str(key),data=vals.astype('float64'),compression='gzip',\n",
    "                                        compression_opts=9,maxshape=(vals.shape[0],None,vals.shape[2]))\n",
    "                else:\n",
    "                    sar_data[str(key)].resize((sar_data[str(key)].shape[1] + vals.shape[1]), axis=1)\n",
    "                    sar_data[str(key)][:,-vals.shape[1]:,:] = vals\n",
    "\n",
    "\n",
    "        dates_unique = sorted(set(sar_dates))   \n",
    "        if flag:\n",
    "            meta.create_dataset('sar_dates',(len(dates_unique)),'S10',\n",
    "                                [d.encode('ascii','ignore') for d in dates_unique],maxshape=(None,))\n",
    "            flag = False\n",
    "        else:\n",
    "            meta['sar_dates'].resize((meta['sar_dates'].shape[0] + len(dates_unique)), axis=0)\n",
    "            meta['sar_dates'][-len(dates_unique):] = dates_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a13d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12321d",
   "metadata": {},
   "source": [
    "## Filtering and Daily Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c764c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### interpolation timestamps\n",
    "\n",
    "def date_range(start, end, intv):\n",
    "\n",
    "    start = datetime.strptime(start,\"%Y-%m-%d\")#+timedelta(days=31)\n",
    "    end = datetime.strptime(end,\"%Y-%m-%d\")-timedelta(days=1)\n",
    "    diff = (end  - start ) / intv\n",
    "    for i in range(intv):\n",
    "        yield (start + diff * i)\n",
    "    yield end\n",
    "\n",
    "s2_temporal_resolution = 5\n",
    "s1_temporal_resolution = 6\n",
    "\n",
    "start = datetime.strptime(start_date,\"%Y-%m-%d\")#+timedelta(days=31)\n",
    "end = datetime.strptime(end_date,\"%Y-%m-%d\")-timedelta(days=1)\n",
    "d_break = (end-start).days//s2_temporal_resolution\n",
    "d_break_sar = (end-start).days//s1_temporal_resolution\n",
    "    \n",
    "dates_interp = list(date_range(start_date,end_date,d_break))\n",
    "dates_interp = [d.date() for d in dates_interp]\n",
    "dates_interp_sar = list(date_range(start_date,end_date,d_break_sar))\n",
    "dates_interp_sar = [d.date() for d in dates_interp_sar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd44760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_interpolation(vals,dates_timestamp,sar=False,smoothing=False):\n",
    "\n",
    "    df_band = pd.DataFrame(data=vals.T.copy(),columns=dates_timestamp)\n",
    "    #     df_band[df_band==-9999] = np.nan\n",
    "    start = datetime.strptime(start_date,\"%Y-%m-%d\")-timedelta(days=1)\n",
    "    end = datetime.strptime(end_date,\"%Y-%m-%d\")+timedelta(days=1)\n",
    "    df_band[start] = np.nan\n",
    "    df_band[end] = np.nan\n",
    "    df_band = df_band[sorted(df_band.columns)]\n",
    "    df_band = df_band.resample('D',axis=1).mean().interpolate('linear',axis=1).bfill(axis=1).ffill(axis=1)\n",
    "    if sar:\n",
    "        df_band = df_band[dates_interp_sar]\n",
    "    else:\n",
    "        df_band = df_band[dates_interp]\n",
    "    if smoothing:\n",
    "        df_band = df_band.rolling(window=3,center=True,axis=1).median().bfill(axis=1).ffill(axis=1)\n",
    "    \n",
    "    return df_band.values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce981a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10236/10236 [44:24<00:00,  3.84it/s]\n"
     ]
    }
   ],
   "source": [
    "hf = h5py.File('/home/eouser/Desktop/jason_notebooks/fs/{0}/fs_{0}_{1}_{2}_{3}_raw.h5'.format(pilot,region,year,buffer), \"r\", libver='latest', swmr=True)\n",
    "unique_ids = np.array([str(i.decode('UTF-8')) for i in hf['metadata']['unique_ids'][:]])\n",
    "dates = np.array([str(d.decode('UTF-8')) for d in hf['metadata']['dates'][:]])\n",
    "dates_timestamp = np.array([datetime.strptime(x,\"%Y-%m-%d\") for x in dates])\n",
    "sar_dates = np.array([str(d.decode('UTF-8')) for d in hf['metadata']['sar_dates'][:]])\n",
    "sar_dates_timestamp = np.array([datetime.strptime(x,\"%Y-%m-%d\") for x in sar_dates])\n",
    "bands = np.array([str(b.decode('UTF-8')) for b in hf['metadata']['bands'][:]])\n",
    "sar_bands = np.array([str(b.decode('UTF-8')) for b in hf['metadata']['sar_bands'][:]])\n",
    "ids = np.array(list(hf['data'].keys()))\n",
    "\n",
    "# ids = sorted(set(ids.astype(int)).intersection(set(noa_ids_to_keep)))\n",
    "ids = np.array(ids).astype(str)\n",
    "unique_ids = np.array([noa_to_id[int(i)] for i in ids])\n",
    "\n",
    "ndvi_i = np.where(bands=='ndvi')[0][0]\n",
    "lower_ndvi_thresh = 0 \n",
    "sample = True # put yes if you want to extract a sample of random pixels inside the parcel\n",
    "# sample_size = 0.2 # the number the sample pixels > 1 or the portion <= 1\n",
    "sample_size = 10\n",
    "\n",
    "\n",
    "dates_new = [d.strftime(\"%Y-%m-%d\") for d in dates_interp]\n",
    "sar_dates_new = [d.strftime(\"%Y-%m-%d\") for d in dates_interp_sar]\n",
    "\n",
    "\n",
    "hf_interp = h5py.File('/home/eouser/Desktop/jason_notebooks/fs/{0}/fs_{0}_{1}_{2}_{3}_interp.h5'.format(pilot,region,year,buffer), 'w')\n",
    "data_interp = hf_interp.create_group('data')\n",
    "sar_data_interp = hf_interp.create_group('sar_data')\n",
    "coords_interp = hf_interp.create_group('coords')\n",
    "meta_interp = hf_interp.create_group('metadata')\n",
    "meta_interp.create_dataset('bands',(len(bands)),'S10',[b.encode('ascii','ignore') for b in bands])\n",
    "meta_interp.create_dataset('sar_bands',(len(sar_bands)),'S10',[b.encode('ascii','ignore') for b in sar_bands])\n",
    "meta_interp.create_dataset('dates',(len(dates_new)),'S10',[d.encode('ascii','ignore') for d in dates_new])\n",
    "meta_interp.create_dataset('sar_dates',(len(sar_dates_new)),'S10',[d.encode('ascii','ignore') for d in sar_dates_new])\n",
    "meta_interp.create_dataset('unique_ids',(len(unique_ids)),'S40',[i.encode('ascii','ignore') for i in unique_ids])\n",
    "\n",
    "\n",
    "for i in tqdm(ids):\n",
    "    \n",
    "    vals = hf['data'][i][:]\n",
    "    sar_vals = hf['sar_data'][i][:]\n",
    "    coords = hf['coords'][i][:]\n",
    "    \n",
    "    if sample:\n",
    "        vals_size = vals.shape[-1]\n",
    "        if sample_size <= 1:\n",
    "            sample_size = int(vals_size*sample_size)\n",
    "            s = np.sort(np.random.choice(np.arange(vals_size),sample_size,replace=False))\n",
    "            vals = vals[:,:,s]\n",
    "            sar_vals = sar_vals[:,:,s]\n",
    "            coords = coords[s,:]\n",
    "        else:\n",
    "            vals_size = vals.shape[-1]\n",
    "            if sample_size<=vals_size:\n",
    "                s = np.sort(np.random.choice(np.arange(vals_size),sample_size,replace=False))\n",
    "            else:\n",
    "                s = np.sort(np.random.choice(np.arange(vals_size),sample_size,replace=True))\n",
    "            vals = vals[:,:,s]\n",
    "            sar_vals = sar_vals[:,:,s]\n",
    "            coords = coords[s,:]\n",
    "    \n",
    "    \n",
    "    vals[vals==-9999.] = np.nan\n",
    "    sar_vals[(sar_vals==0.)|(sar_vals<=-30.)] = np.nan\n",
    "    \n",
    "    ii = np.where(vals[ndvi_i,:,:]<=lower_ndvi_thresh) ## put nan for every band if ndvi<=ndvi_threshold\n",
    "    \n",
    "    vals_new = np.zeros((vals.shape[0],len(dates_new),vals.shape[-1]))\n",
    "    sar_vals_new = np.zeros((sar_vals.shape[0],len(sar_dates_new),sar_vals.shape[-1]))\n",
    "    for b in range(len(bands)):\n",
    "        vals[b,ii[0],ii[1]] = np.nan\n",
    "        vals_new[b,:,:] = daily_interpolation(vals[b,:,:],dates_timestamp,sar=False)\n",
    "    for b in range(len(sar_bands)):\n",
    "        sar_vals_new[b,:,:] = daily_interpolation(sar_vals[b,:,:],sar_dates_timestamp,sar=True)\n",
    "\n",
    "    coords_interp.create_dataset(str(i),data=np.array(coords), compression='gzip',compression_opts=9)\n",
    "    data_interp.create_dataset(str(i),data=vals_new.astype('float16'),compression='gzip',compression_opts=9)\n",
    "    sar_data_interp.create_dataset(str(i),data=sar_vals_new.astype('float16'),compression='gzip',compression_opts=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a2e0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()\n",
    "hf_interp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4102a3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
